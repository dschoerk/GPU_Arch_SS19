\section{Results and Comparison}

In this chapter we are describing how we have tested and benchmarked our implementations. We have tested the different flavors of the algorithm on two different test systems. The first system is the department's test system with a Tesla K10.G2 GPU, and the second system is a personal computer with an NVIDIA RTX 2070 GPU.

We evaluate the implemented approaches by providing benchmarks with different sample and window sizes and discuss the differences between the chosen strategies as well as differences regarding the two GPU's.

\subsection{NVIDIA RTX 2070}

The overview in Figure~\ref{fig:group_picture_rtx2070} shows the output of NVIDIA's visual profiler (NVVP) executed on the RTX 2070 test system. The green bars are marking the full computation time for each algorithm. This includes the allocation of host memory (unmarked).

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/group_picture_rtx2070}
    \caption{Overview of algorithms for a profiling run with a sample size of $10^7$ and a sliding window size of 256 on the NVIDIA RTX 2070. NVTX ranges are marking the phases of each algorithm. Preparation time in blue. Kernel execution in yellow. Copying results to host in pink. Shown from left to right are: CUDA Malloc, CUDA Page-Locked, CUDA Page-Locked Shared, Thrust Naive, Thrust, and CUDA Tiled}
    \label{fig:group_picture_rtx2070}
\end{sidewaysfigure}

The allocation and transfer of/to device memory (blue), the kernel execution (yellow) and transferring the results from device to host memory (pink).

Without going into the detailed numbers, we can see that the time taken for memory allocations and transfer dominates the timescale. Typically the kernel execution only takes a quarter of the overall processing time (except for the thrust implementation). 

What we can also see in this overview is that multiple kernel calls are computation wise extremely expensive, but sometimes needed to achieve a grid wide synchronization as required in the Thrust implementation (Figure~\ref{fig:group_picture_rtx2070} \#5). Multiple kernels thus should be avoided at all cost, and be replaced by tiling schemes which limit the requirements for synchronization to sub groups of blocks or even warps as used in the CUDA Tiled approach (Figure~\ref{fig:group_picture_rtx2070} \#6). 

Figure~\ref{fig:rtx2070_ms_vs_samples} shows the benchmark results for the NVIDIA RTX 2070 GPU with window sizes of 100, 500, 800 and 1000 elements. We left out Lemire's approach for a better scaling of the graph. The clearly worst performing algorithm of ours was the Thrust implementation. Since global synchronization via multiple sequential kernel starts was required, the performance of this approach took a big hit. The other five approaches are performing very similarly. For smaller window sizes (100, 500) the CUDA Tiled approach performs fastest by a small margin. For larger windows (800, 1000) the naive iteration scheme performs better, and among these CUDA Page-Locked Shared is always the fastest one.

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/rtx2070_ms_vs_samples.png}
    \caption{Results for all six implementations at different window and sample sizes. We omitted Lemire as it is much slower than competitors. Duration is shown on a logarithmic scale. The \texttt{cuda\_tiled} approach appears to be very efficient for large sample sizes and small window sizes, the naive linear scan used by \texttt{cuda\_pagelocked\_shared} dominates at large window sizes.}
    \label{fig:rtx2070_ms_vs_samples}
\end{figure}

\subsection{NVIDIA Tesla K10.G2}

The overview in Figure~\ref{fig:group_picture_tesla_k10g2} shows the output of the NVIDIA's visual profiler (NVVP) executed on the Tesla K10.G2 test system. Similar to the overview for the RTX 2070 test system the green bars are marking the full computation time for each algorithm. Again this includes the allocation of host memory (unmarked). The allocation and transfer of/to device memory (blue), the kernel execution (yellow) and transferring the results from device to host memory (pink).

\begin{sidewaysfigure}
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/group_picture_tesla_k10g2}
    \caption{Overview of algorithms for a profiling run with a sample size of $10^7$and a sliding window size of 256 on the NVIDIA Tesla K10.G2. NVTX ranges are marking the phases of each algorithm. Preparation time in blue. Kernel execution in yellow. Copying results to host in pink. Shown from left to right are: CUDA Malloc, CUDA Page-Locked, CUDA Page-Locked Shared, Thrust Naive, Thrust, and CUDA Tiled}
    \label{fig:group_picture_tesla_k10g2}
\end{sidewaysfigure}

An obvious overall difference is that all measured times on the 
Tesla K10.G2 test system are substantially (factor 2-3) longer than those measured on the RTX 2070 test system, which is not surprising considering the hardware specifications of both. Thus we'll now focus on relative differences between the two systems.

In contrast to the results from RTX 2070 test system the time required for memory allocation and memory transfer is not at all that dominating. However, the direct access to page-locked memory on the host CPU (which manifests itself in the kernel execution time of the CUDA Page-Locked implementation) dominates the timing by at least an order of magnitude. A wild guess (which is not confirmed) is that the Tesla K10.G2 test system is inferior when it comes to coalescing multiple memory accesses into a single bulk transfer. Another interesting difference is that the Thrust implementation is en-par with both plain CUDA implementations using shared memory for caching (which suggests that on the Tesla K10.G2 test system the Thrust library performs a similar caching under the hood). A final difference is that the Thrust implementation outperforms Thrust Naive implementation  on the Tesla K10.G2 test system whereas on the RTX 2070 test system it's exactly the other way round.

\begin{figure}[p]
    \centering
    \includegraphics[width=1.0\linewidth]{Figures/tesla_k10g2_ms_vs_samples.png}
    \caption{Results for all six implementations at different window and sample sizes. Again we omitted Lemire as it is considerably slower than competitors with large sample sets. Duration is shown on a logarithmic scale. The \texttt{cuda\_pagelocked} approach (using the naive linear scan) is non-performing for all sample and small window sizes. The \texttt{cuda\_pagelocked\_shared} (which uses the naive linear scan as well) dominates at large window sizes.}
    \label{fig:tesla_k10g2_ms_vs_samples}
\end{figure}

Figure~\ref{fig:tesla_k10g2_ms_vs_samples} shows the benchmark results for the NVIDIA Tesla K10.G2 GPU with window sizes of 100, 500, 800 and 1000 elements. Again we left out Lemire's approach for a better scaling of the graph. The clearly worst performing algorithm on this test system was the CUDA Page-Locked implementation (without using shared memory as a cache) for all sample and sliding windows sizes. The hypothesized cause of this is the lack of capability to coalesce multiple memory accesses into a single bulk transfer. The CUDA tiled implementation showed the second worst performance for small sample sizes, but gains on larger samples sizes as long as the window size is below a threshold of approx.\ 900. With larger sample sizes the CUDA Page-Locked Shared implementation clearly shows the best performance.

\section{Summary \& Conclusion}

In the scope of this lab, we implemented several different flavors of a parallel version of a sliding window minimum/maximum filter with configurable window size using NVIDIAâ€™s parallel computing platform and programming  model  for general computing on graphical processing units (GPUs) named  CUDA. These various implementation exploited different strategies w.r.t.\ performance optimizations. We conducted comprehensive measurements with different sample sizes and various sizes of the sliding window on two different hardware platforms.

The comparison of the measurement results were not fully conclusive across these two hardware platforms meaning that optimization strategies that are beneficial for one hardware platform were counter productive for the other. One common outcome on both hardware platforms however was that a rather naive parallelized linear scan together with the use of page-locked host memory combined with shared GPU memory as a program controlled cache performed superior for larger sample sizes and all window sizes on both hardware platforms.

The substantial differences in the measurement results on the two hardware platforms w.r.t.\ the merit of certain optimization strategies clearly suggest that pursuing a hybrid approach, i.e., providing various different kernel implementations within a single executable and selecting the appropriate one during run-time based on the concrete workload and the concrete hardware platform, is feasible and beneficial when striving for optimal performance on a broad range of different hardware platforms and when dealing with various different workloads.
